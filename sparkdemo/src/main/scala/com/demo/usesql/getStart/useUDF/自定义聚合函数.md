## User Defined Aggregate Functions (UDAFs)

```html
(用户自定义聚合函数)
一次作用与多行，最后返回单指
文档描述了如何需要创造和注册类
```

```html
Description
User-Defined Aggregate Functions (UDAFs) are user-programmable routines that act on multiple rows at once 
and return a single aggregated value as a result. 
This documentation lists the classes that are required for creating and registering UDAFs. 
It also contains examples that demonstrate how to define and register UDAFs in Scala and invoke them in Spark SQL.
```
```html
用户定义聚合的基类，可在数据集操作中使用它获取所有元素的值，并将其减少为单个值。
一些值:
in:
buf:聚合的中间值
out:最后输出的结果

涉及到的方法：
bufferEncoder：明确编码的中间值类型
finish：转换聚合后的输出值
merge：聚合两个中间值
outputEncoder：明确输出值的类型
reduce：聚合：中间值和输入值的聚合，输出修改后的中间值
zero：中间值的初始值
```

```HTML
Aggregator[-IN, BUF, OUT]
A base class for user-defined aggregations, which can be used in Dataset operations to take all of the elements 
of a group and reduce them to a single value.
    IN - The input type for the aggregation.
    BUF - The type of the intermediate value of the reduction.
    OUT - The type of the final output result.
    
        • bufferEncoder: Encoder[BUF]
        Specifies the Encoder for the intermediate value type.
        • finish(reduction: BUF): OUT
        Transform the output of the reduction.
        • merge(b1: BUF, b2: BUF): BUF
        Merge two intermediate values.
        • outputEncoder: Encoder[OUT]
        Specifies the Encoder for the final output value type.
        • reduce(b: BUF, a: IN): BUF
        Aggregate input value a into current intermediate value. For performance, the function may 
        modify b and return it instead of constructing new object for b.
        • zero: BUF
        The initial value of the intermediate result for this aggregation.
```

## Examples

```html
类型安全的UDAF
针对强类型数据集是通过继承Aggregator开始的
例如：求平均值
例子中可以看到不能通过建立视图的方式进行sql语句的查询
因为输入是一行的数据，不是单列进行聚合。
```

```html
Type-Safe User-Defined Aggregate Functions
User-defined aggregations for strongly typed Datasets revolve around 
the Aggregator abstract class. For example, a type-safe user-defined average can look like:
```



```shell script
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import org.apache.spark.sql.expressions.Aggregator
case class Employee(name: String, salary: Long)
case class Average(var sum: Long, var count: Long)
object MyAverage extends Aggregator[Employee, Average, Double] {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  def zero: Average = Average(0L, 0L)
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  def reduce(buffer: Average, employee: Employee): Average = {
    buffer.sum += employee.salary
    buffer.count += 1
    buffer
  }
  // Merge two intermediate values
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  // Transform the output of the reduction
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
  // Specifies the Encoder for the intermediate value type
  def bufferEncoder: Encoder[Average] = Encoders.product
  // Specifies the Encoder for the final output value type
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}
val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
ds.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+
// Convert the function to a `TypedColumn` and give it a name
val averageSalary = MyAverage.toColumn.name("average_salary")
val result = ds.select(averageSalary)
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+

```

```html
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala" in the Spark repo.

```


### Untyped User-Defined Aggregate Functions
```html
Typed aggregations, as described above, may also be registered as untyped aggregating  
UDFs for use with DataFrames. For example, a user-defined average for untyped DataFrames can look like:
```

```shell script
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.expressions.MutableAggregationBuffer
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
import org.apache.spark.sql.types._

object MyAverage extends UserDefinedAggregateFunction {
  // Data types of input arguments of this aggregate function
  def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)
  // Data types of values in the aggregation buffer
  def bufferSchema: StructType = {
    StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)
  }
  // The data type of the returned value
  def dataType: DataType = DoubleType
  // Whether this function always returns the same output on the identical input
  def deterministic: Boolean = true
  // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to
  // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides
  // the opportunity to update its values. Note that arrays and maps inside the buffer are still
  // immutable.
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }
  // Updates the given aggregation buffer `buffer` with new input data from `input`
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      buffer(0) = buffer.getLong(0) + input.getLong(0)
      buffer(1) = buffer.getLong(1) + 1
    }
  }
  // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }
  // Calculates the final result
  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
}

// Register the function to access it
spark.udf.register("myAverage", MyAverage)

val df = spark.read.json("examples/src/main/resources/employees.json")
df.createOrReplaceTempView("employees")
df.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+

```

```html
Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala" 
in the Spark repo.
```
Related Statements
• Scalar User Defined Functions (UDFs)
• Integration with Hive UDFs/UDAFs/UDTFs
