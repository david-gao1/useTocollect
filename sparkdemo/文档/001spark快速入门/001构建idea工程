###建立idea工程
* 新建maven模块
* 设置scalaSDK：官网下载，全局设置，导入下载的位置
* 创建scala文件夹，并设置为root source
![设置sdk](附件/设置SDK.png)
*  引入spark依赖
```xml
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>2.4.3</version>
        </dependency>
```
![安装spark](附件/spark依赖.png)

* 编写wc代码，测试运行
```scala
import org.apache.spark.{SparkConf, SparkContext}

object WordCountScala {

  def main(args: Array[String]): Unit = {
    //第一步：创建SparkContext
    val conf = new SparkConf()
    conf.setAppName("WordCountScala")//设置任务名称
      .setMaster("local")//local表示在本地执行
    val sc = new SparkContext(conf)

    //第二步：加载数据
    val linesRDD = sc.textFile("文档/训练资料/wc")

    //第三步：对数据进行切割，把一行数据切分成一个一个的单词
    val wordsRDD = linesRDD.flatMap(_.split(" "))

    //第四步：迭代words,将每个word转化为(word,1)这种形式
    val pairRDD = wordsRDD.map((_,1))

    //第五步：根据key(其实就是word)进行分组聚合统计
    val wordCountRDD = pairRDD.reduceByKey(_ + _)

    //第六步：将结果打印到控制台
    wordCountRDD.foreach(wordCount=>println(wordCount._1+"--"+wordCount._2))

    //第七步：停止SparkContext
    sc.stop()
  }

}
```