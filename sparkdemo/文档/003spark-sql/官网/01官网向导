#Spark SQL, DataFrames and Datasets Guide

向导
```html
spark-sql是用来处理结构化数据的spark模块.
可以通过sql和dataset api实现与spark-sql的交互。
计算结果时，使用相同的执行引擎，与您使用的API/语言无关。这种统一意味着开发人员可以轻松地在不同的api之间来回切换。
```


```html
Spark SQL is a Spark module for structured data processing. 
Unlike the basic Spark RDD API, 
the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. 
Internally, Spark SQL uses this extra information to perform extra optimizations. 
There are several ways to interact with Spark SQL including SQL and the Dataset API. 
When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. 
This unification means that developers can easily switch back and forth between different APIs 
based on which provides the most natural way to express a given transformation.

All of the examples on this page use sample data included in the Spark distribution 
and can be run in the spark-shell, pyspark shell, or sparkR shell.
```

#SQL
```html
spark sql也从hive中读取数据
使用另外一种编程语言运行sql时，返回的将是dataset或dataframe。
也可以通过命令行或者JDBC/ODBC去和sql进行交互。
```

```html
One use of Spark SQL is to execute SQL queries. 
Spark SQL can also be used to read data from an existing Hive installation. 
For more on how to configure this feature, please refer to the Hive Tables section. 
When running SQL from within another programming language the results will be returned as a Dataset/DataFrame. 
You can also interact with the SQL interface using the command-line or over JDBC/ODBC.
```


#Datasets and DataFrames
```html
dataset
是数据的分布式集合
Dataset是Spark 1.6中添加的一个新接口，它提供了RDDs的优点(强类型、使用强大lambda函数的能力) 利用Spark SQL优化的执行引擎的优点。
dataset可以由JVM对象构造，然后使用函数转换(map、flatMap、filter等)操作。
支持java或scala
python不支持dataset api，但是由于python的一些动态属性，一些dataset api也已经支持

DataFrame 
DataFrame 是组成指定列的数据集
概念上等同与关系型数据库的表或R/Python的数据框架，但在内部具有更丰富的优化。

DataFrames可以由结构化的数据文件，hive表，外部的数据库，或已存在的rdd。

dataframe api可以由Scala, Java, Python和R获取

在scala和java中，dataframe由dataset的行表示

在Scala API中，DataFrame只是Dataset[Row]的类型别名。
而在Java API中，用户需要使用dataset<Row>表示一个dataframe。
```
```html
A Dataset is a distributed collection of data. 
Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) 
with the benefits of Spark SQL’s optimized execution engine. 
A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). 
The Dataset API is available in Scala and Java. 
Python does not have the support for the Dataset API. 
But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available 
(i.e. you can access the field of a row by name naturally row.columnName). 
The case for R is similar.

A DataFrame is a Dataset organized into named columns. 
It is conceptually equivalent to a table in a relational database or a data frame in R/Python, 
but with richer optimizations under the hood. 
DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. 
The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. 
In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.

Throughout this document, we will often refer to Scala/Java Datasets of Rows as DataFrames.
```